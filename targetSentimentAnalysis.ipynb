{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "from wordExtractor import wordExtractor\n",
    "from dictConfig import replaceDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def log(*k, **kw):\n",
    "    timeStamp = time.strftime('[%y-%m-%d %H:%M:%S] ', time.localtime())\n",
    "    print(timeStamp, end='')\n",
    "    print(*k, **kw)\n",
    "def debug(*k, stop=False, **kw):\n",
    "    print('[DEBUG] ', end='')\n",
    "    print(*k, **kw)\n",
    "    if stop:\n",
    "        raise SystemExit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPara(module, useString=True):\n",
    "    para = sum([x.nelement() for x in module.parameters()])\n",
    "    if not useString:\n",
    "        return para\n",
    "    elif para >= 2**20:\n",
    "        return '{:.2f}M'.format(para / 2**20)\n",
    "    elif para >= 2**10:\n",
    "        return '{:.2f}K'.format(para / 2**10)\n",
    "    else:\n",
    "        return str(para)\n",
    "def savemodel(name, model):\n",
    "    torch.save(model.state_dict(), name + '.pt')\n",
    "def loadmodel(name, model):\n",
    "    model.load_state_dict(torch.load(name + '.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用<>包夹表示特殊词，不作处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojiDict = {\n",
    "    ':)': '<smile>',\n",
    "    ':D': '<smile>',\n",
    "    ':.(': '<cry>',\n",
    "    '-.-': '<emoji1>',\n",
    "    '-__-': '<emoji2>',\n",
    "    '>.>': '<emoji3>',\n",
    "    'O.o': '<emoji4>',\n",
    "    ':??': '<emoji4>'\n",
    "}\n",
    "chineseDict = {\n",
    "    '\\u2019': \"'\",\n",
    "    '\\u002c': ',',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = wordExtractor()\n",
    "extractor.addDict(replaceDict)\n",
    "\n",
    "stemmer = nltk.stem.LancasterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "stopSet = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperateSpecialElement(wordsTargetList):\n",
    "    i = 0\n",
    "    while i != len(wordsTargetList):\n",
    "        word, isTarget = wordsTargetList[i]\n",
    "        s = re.search('<\\w*>', word)\n",
    "        if s != None:\n",
    "            wordsTargetList.pop(i)\n",
    "            one = word[:s.start()]\n",
    "            two = word[s.start():s.end()]\n",
    "            three = word[s.end():]\n",
    "            if len(one) != 0:\n",
    "                wordsTargetList.insert(i, (one, isTarget))\n",
    "                i += 1\n",
    "            wordsTargetList.insert(i, (two, isTarget))\n",
    "            i += 1\n",
    "            if len(three) != 0:\n",
    "                wordsTargetList.insert(i, (three, isTarget))\n",
    "        else:\n",
    "            i += 1\n",
    "    return wordsTargetList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeFullStop(wordsTargetList):\n",
    "    wordsTargetPureList = []\n",
    "    for word, isTarget in wordsTargetList:\n",
    "        newList = [(newW, isTarget) for newW in re.split('[.,?!]', word) if len(newW) != 0]\n",
    "        wordsTargetPureList.extend(newList)\n",
    "    return wordsTargetPureList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word, pos):\n",
    "    if pos == 'NN':\n",
    "        return lemmatizer.lemmatize(word, pos='n')\n",
    "    if pos == 'VB':\n",
    "        return lemmatizer.lemmatize(word, pos='v')\n",
    "    if pos == 'JJ':\n",
    "        return lemmatizer.lemmatize(word, pos='a')\n",
    "    if pos == 'R':\n",
    "        return lemmatizer.lemmatize(word, pos='r')\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转小写\n",
    "# -> 展开缩写  \n",
    "# -> 空格分割  \n",
    "# -> 标记@网名 -> 标记#topic  \n",
    "# -> 标记网址  \n",
    "# -> 表情包:) :D :.(  \n",
    "# -> 标记target  \n",
    "# -> 删除\".,\"，断句  \n",
    "# ? -> 标记无用词  \n",
    "# ? -> 删除  \n",
    "# x -> 转词根  \n",
    "# x -> 词性标准 -> 词性还原  \n",
    "# x -> 停词\n",
    "\n",
    "# 返回处理后的文本和target\n",
    "def processSentence(text, target):\n",
    "    # 转小写\n",
    "    lowerText = text.lower()\n",
    "    # 移除中文字符\n",
    "    for c in chineseDict:\n",
    "        lowerText = lowerText.replace(c, chineseDict[c])\n",
    "    # 展开缩写\n",
    "    expendText = extractor.expand_contractions(lowerText)\n",
    "\n",
    "    # 空格分割\n",
    "    wordsList = expendText.split()\n",
    "    # 标记@网名\n",
    "    wordsList = [re.sub(r'@[\\w_]*', '<user>', word) for word in wordsList]\n",
    "    # 标记#topic\n",
    "    wordsList = [re.sub(r'#\\w*', '<topic>', word) for word in wordsList]\n",
    "    # 标记网址\n",
    "    wordsList = [re.sub(r'http(s)?://.*', '<web>', word) for word in wordsList]\n",
    "    # 标记省略号\n",
    "    wordsList = [re.sub(r'\\.{3,}', '<ellipsis>', word) for word in wordsList]\n",
    "    # 标记数字\n",
    "    wordsList = [re.sub(r'\\d+\\.?\\d*', '<number>', word) for word in wordsList]\n",
    "    # 标记表情包\n",
    "    for emoji in emojiDict:\n",
    "        wordsList = [word.replace(emoji.lower(), emojiDict[emoji]) for word in wordsList]\n",
    "\n",
    "    # 标记target\n",
    "    wordsTargetList = [(word, target[0] <= i <= target[1]) for i, word in enumerate(wordsList)]\n",
    "    # 分离特殊符号<***>\n",
    "    wordsTargetList = seperateSpecialElement(wordsTargetList)\n",
    "    # 删除\".,\"，断句\n",
    "    wordsTargetList = removeFullStop(wordsTargetList)\n",
    "\n",
    "    # -> 词性标准 -> 词性还原 \n",
    "    # pureText, isTargets = zip(*wordsTargetList)\n",
    "    # _, pos_tags = zip(*nltk.pos_tag(pureText))\n",
    "\n",
    "    # oriText = [lemmatize(word, pos=pos) for word, pos in zip(pureText, pos_tags)]\n",
    "    # oriText = [stemmer.stem(word) for word in oriText]\n",
    "\n",
    "    # wordsTargetList = list(zip(oriText, isTargets))\n",
    "\n",
    "    # 去除停用词\n",
    "    # wordsTargetList = [(word, isTarget) for word, isTarget in wordsTargetList if word not in stopSet]\n",
    "    \n",
    "    # 加上结束符\n",
    "    wordsTargetList.append(('<EOS>', False))\n",
    "    finText, isTargets = zip(*wordsTargetList)\n",
    "\n",
    "    # 恢复target\n",
    "    beg = -1\n",
    "    end = -1\n",
    "    for i, isTarget in enumerate(isTargets):\n",
    "        if isTarget and beg == -1:\n",
    "            beg = i\n",
    "        if not isTarget and beg != -1:\n",
    "            end = i - 1\n",
    "            break\n",
    "            \n",
    "    return finText, (beg, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Mrf Don't hesitate  to ask questions. Be positive,Keep positive. Emm:D:DHMm.... they'll be selected\n",
      "(('<user>', 'do', 'not', 'hesitate', 'to', 'ask', 'questions', 'be', 'positive', 'keep', 'positive', 'emm', '<smile>', '<smile>', 'hmm', '<ellipsis>', 'they', 'will', 'be', 'selected'), (3, 5))\n",
      "\n",
      "Yes, I intend to go everyweek, but of course that’s not confirmed.Hope you get well soon.May you be free from physical sufferin.\n",
      "(('yes', 'i', 'intend', 'to', 'go', 'everyweek', 'but', 'of', 'course', 'that', 'is', 'not', 'confirmed', 'hope', 'you', 'get', 'well', 'soon', 'may', 'you', 'be', 'free', 'from', 'physical', 'sufferin'), (3, 5))\n",
      "\n",
      "Oh... I c i c... Haha, xin ask u to take away food 4 her, pau or smethg... Gee, can help me buy packet milo? Thanx sweety...\n",
      "(('oh', '<ellipsis>', 'i', 'see', 'i', 'see', '<ellipsis>', 'hah', 'xin', 'ask', 'uou', 'to', 'take', 'away', 'food', '<number>', 'her', 'pau', 'or', 'smethg', '<ellipsis>', 'gee', 'can', 'help', 'me', 'buy', 'packet', 'milo', 'thanx', 'sweety', '<ellipsis>'), (4, 7))\n",
      "\n",
      "#Vote4UrDAYUMSelf Comedy Show w/ @Ali_Speaks TOMORROW @ the Houston Improv, 7:30 PM. FREE SHOW! Get... http://t.co/by9COiwf\n",
      "(('<topic>', 'comedy', 'show', 'w/', '<user>', 'tomorrow', '<user>', 'the', 'houston', 'improv', '<number>', ':', '<number>', 'pm', 'free', 'show', 'get', '<ellipsis>', '<web>'), (3, 5))\n",
      "\n",
      "Gas by my house hit $3.39!!!! OK.I'm going to Chapel Hill on Sat. :)\n",
      "(('gas', 'by', 'my', 'house', 'hit', '$', '<number>', 'ok', 'i', 'am', 'going', 'to', 'chapel', 'hill', 'on', 'sat', '<smile>'), (3, 6))\n",
      "\n",
      "`` Thud '' means it is just a boring wordcount post .\n",
      "(('``', 'thud', \"''\", 'means', 'it', 'is', 'just', 'a', 'boring', 'wordcount', 'post'), (3, 5))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"@Mrf Don't hesitate  to ask questions. Be positive,Keep positive. Emm:D:DHMm.... they'll be selected\",\n",
    "    'Yes\\u002c I intend to go everyweek\\u002c but of course that\\u2019s not confirmed.Hope you get well soon.May you be free from physical sufferin.',\n",
    "    'Oh... I c i c... Haha\\u002c xin ask u to take away food 4 her\\u002c pau or smethg... Gee\\u002c can help me buy packet milo? Thanx sweety...',\n",
    "    '#Vote4UrDAYUMSelf Comedy Show w/ @Ali_Speaks TOMORROW @ the Houston Improv, 7:30 PM. FREE SHOW! Get... http://t.co/by9COiwf',\n",
    "    \"Gas by my house hit $3.39!!!! OK.I'm going to Chapel Hill on Sat. :)\",\n",
    "    \"`` Thud '' means it is just a boring wordcount post .\",\n",
    "]\n",
    "for text in texts:\n",
    "    print(text)\n",
    "    print(processSentence(text, (3, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词性还原\n",
    "https://blog.csdn.net/qq_16234613/article/details/79430381"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class corpusData():\n",
    "    def __init__(self, table, closeThreshold=10):\n",
    "        self.threshold = closeThreshold\n",
    "        self.corpus = self.loadCorpus(table)\n",
    "        self.processedCorpus = self.processe(self.corpus)\n",
    "        self.word2index, self.index2word = self.getDict(self.processedCorpus)\n",
    "    def loadCorpus(self, table):\n",
    "        corpus = []\n",
    "        \n",
    "        polarityClass = {\n",
    "            'positive': 0,\n",
    "            'neutral': 1,\n",
    "            'negative': 2,\n",
    "            'unknwn': 3\n",
    "        }\n",
    "        \n",
    "        for _, line in table.iterrows():\n",
    "            target = (line['index1'], line['index2'])\n",
    "            polarity = polarityClass[line['polarity']]\n",
    "            text = line['text']\n",
    "            corpus.append((text, target, polarity))\n",
    "        return corpus\n",
    "    def getDict(self, corpus):\n",
    "        fdist = nltk.probability.FreqDist()\n",
    "        for words, _, _ in corpus:\n",
    "            fdist.update(words)\n",
    "        index2word = [word for word, freq in fdist.most_common() if freq > self.threshold]\n",
    "        index2word.append('<unknown>')\n",
    "        word2index = { word: i for i, word in enumerate(index2word) }\n",
    "        return word2index, index2word\n",
    "    def processe(self, corpus):\n",
    "        processedCorpus = []\n",
    "        for text, target, polarity in corpus:\n",
    "            newText, newTarget = processSentence(text, target)\n",
    "            processedCorpus.append((newText, newTarget, polarity))\n",
    "        return processedCorpus\n",
    "    def sentence2Tensor(self, sentence):\n",
    "        change = lambda word: word if word in self.word2index else '<unknown>'\n",
    "        return torch.tensor([self.word2index[change(word)] for word in sentence])\n",
    "    def tensor2Sentence(self, tensor, join=True):\n",
    "        if join:\n",
    "            return ' '.join([self.index2word[i] for i in tensor])\n",
    "        else:\n",
    "            return [self.index2word[i] for i in tensor]\n",
    "    def getRawText(self, index):\n",
    "        return self.corpus[index][0]\n",
    "    def getProcessedText(self, index, join=True):\n",
    "        if join:\n",
    "            return ' '.join(self.processedCorpus[index][0])\n",
    "        else:\n",
    "            return self.processedCorpus[index][0]\n",
    "    def __getitem__(self, index):\n",
    "        text = self.getProcessedText(index, join=False)\n",
    "        _, target, polarity = self.processedCorpus[index]\n",
    "        return self.sentence2Tensor(text), target, polarity\n",
    "    def __len__(self):\n",
    "        return len(self.processedCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迭代样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_corpus(batch):\n",
    "    with torch.no_grad():\n",
    "        texts, targets, polaritys = zip(*batch)\n",
    "        maxlen = max([len(text) for text in texts])\n",
    "        batchTexts = [torch.nn.functional.pad(text, pad=(0, maxlen-len(text))) for text in texts]\n",
    "        batchTexts = torch.stack(batchTexts)\n",
    "    return batchTexts, torch.tensor(targets), torch.tensor(polaritys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试集、验证集的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv('target-sentiment-analysis/train.tsv', sep='\\t', \n",
    "            names=['ID1','ID2','index1','index2', 'polarity','text'])\n",
    "\n",
    "table.sample(frac=1)\n",
    "TRAIN_SIZE = int(len(table)*0.9)\n",
    "tableTrain = table[:TRAIN_SIZE]\n",
    "tableValdate = table[TRAIN_SIZE:]\n",
    "\n",
    "trainSet = corpusData(table=tableTrain, closeThreshold=0)\n",
    "valSet = corpusData(table=tableValdate, closeThreshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainSet, batch_size=5, collate_fn=collate_fn_corpus, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valSet, batch_size=5, collate_fn=collate_fn_corpus, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1040"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 29])\n",
      "torch.Size([5, 22])\n",
      "torch.Size([5, 40])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 19])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 19])\n",
      "torch.Size([5, 31])\n",
      "torch.Size([5, 23])\n",
      "torch.Size([5, 44])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 31])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 31])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([5, 22])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 21])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 31])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 75])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 31])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 39])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([5, 42])\n",
      "torch.Size([5, 38])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 35])\n",
      "torch.Size([5, 35])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 39])\n",
      "torch.Size([5, 41])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 36])\n",
      "torch.Size([5, 36])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 37])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 35])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 31])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 38])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([5, 39])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 31])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 23])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 39])\n",
      "torch.Size([5, 37])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 23])\n",
      "torch.Size([5, 37])\n",
      "torch.Size([5, 34])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 44])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 31])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 41])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 38])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 35])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 36])\n",
      "torch.Size([5, 41])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 34])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 21])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 36])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 22])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 35])\n",
      "torch.Size([5, 41])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 37])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 37])\n",
      "torch.Size([5, 38])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 60])\n",
      "torch.Size([5, 35])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 40])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([5, 38])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 26])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 35])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 39])\n",
      "torch.Size([5, 25])\n",
      "torch.Size([5, 23])\n",
      "torch.Size([5, 45])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 33])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5, 28])\n",
      "torch.Size([5, 23])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 27])\n",
      "torch.Size([5, 31])\n"
     ]
    }
   ],
   "source": [
    "for texts, targets, polarity in valloader:\n",
    "    print(texts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_func, optimizer, trainloader, device):\n",
    "    \"\"\"\n",
    "    train model using loss_fn and optimizer in an epoch.\n",
    "    model: CNN networks\n",
    "    train_loader: a Dataloader object with training data\n",
    "    loss_func: loss function\n",
    "    device: train on cpu or gpu device\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    trainAccuracy = 0\n",
    "    trainLoss = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (images, targets) in enumerate(trainloader):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = loss_func(outputs, targets)\n",
    "        trainLoss += loss.item()\n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # AdamW - https://zhuanlan.zhihu.com/p/38945390\n",
    "        for group in optimizer.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.data = param.data.add(-weight_decay * group['lr'], param.data)\n",
    "\n",
    "        # return the maximum value of each row of the input tensor in the \n",
    "        # given dimension dim, the second return vale is the index location\n",
    "        # of each maxium value found(argmax)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        trainAccuracy += (predicted == targets).sum().item()\n",
    "        \n",
    "        total += len(images)\n",
    "    trainAccuracy /= total\n",
    "    trainLoss /= total\n",
    "    return trainLoss, trainAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, lossFunction, validateloader, device):\n",
    "    # evaluate the model\n",
    "    model.eval()\n",
    "    # context-manager that disabled gradient computation\n",
    "    with torch.no_grad():\n",
    "        # =============================================================\n",
    "        valAccuracy = 0\n",
    "        valLoss = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (images, targets) in enumerate(validateloader):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = lossFunction(outputs, targets)\n",
    "            valLoss += loss.item()\n",
    "            \n",
    "            # return the maximum value of each row of the input tensor in the \n",
    "            # given dimension dim, the second return vale is the index location\n",
    "            # of each maxium value found(argmax)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            valAccuracy += (predicted == targets).sum().item()\n",
    "            \n",
    "            total += len(images)\n",
    "        valAccuracy /= total\n",
    "        valLoss /= total\n",
    "    return valLoss, valAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showCurve(list_trainLoss, list_trainAccuracy, list_valLoss, list_valAccuracy):\n",
    "    xAxis = list(range(len(list_trainLoss)))\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "    axs[0].plot(xAxis, list_trainLoss, label='train')\n",
    "    axs[0].plot(xAxis, list_valLoss, label='validation')\n",
    "    axs[0].set_title('Loss')\n",
    "\n",
    "    axs[1].plot(xAxis, list_trainAccuracy, label='train')\n",
    "    axs[1].plot(xAxis, list_valAccuracy, label='validation')\n",
    "    axs[1].set_title('Accuracy')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.axis()\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('{}'.format(ax.get_title()))\n",
    "        ax.legend()\n",
    "    fig.set_size_inches((8, 4))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "model = model.to(device)\n",
    "\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "num_epoches = 500\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.5)\n",
    "\n",
    "results_train = []\n",
    "results_val = []\n",
    "\n",
    "log('开始训练')\n",
    "for epoch in range(num_epoches):\n",
    "    res_train = train(model, loss_func, optimizer, trainloader, device)\n",
    "    res_val = validate(model, loss_func, trainloader, validateloader, device)\n",
    "    \n",
    "    (trainLoss, trainAccuracy) = res_train\n",
    "    (valLoss, valAccuracy) = res_val\n",
    "    results_train.append(res_train)\n",
    "    results_val.append(res_val)\n",
    "    log('[{:2d}/{}] Loss (Train: {:.6f}, Validation: {:.6f})     Accuracy (Train: {:.4f}, Validation: {:.4f})'\n",
    "              .format(epoch+1, num_epoches, trainLoss, valLoss, trainAccuracy, valAccuracy))\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "showCurve(*zip(*results_train), *zip(*results_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
